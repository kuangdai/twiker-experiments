{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f52dd73-8aa4-4fae-91b2-cf244423d83d",
   "metadata": {},
   "source": [
    "# Load books"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8ddc3bd-ea79-4690-9650-70d22dd38d36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:49:55.505576Z",
     "start_time": "2024-08-16T10:49:55.300788Z"
    }
   },
   "source": [
    "import glob\n",
    "import os\n",
    "books_directory = '/home/ubuntu/Downloads/twiker-experiments/process_analysis/Stephen/books'\n",
    "books = []\n",
    "\n",
    "# List all .txt files in the directory using glob\n",
    "book_files = glob.glob(os.path.join(books_directory, '*.txt'))\n",
    "# Iterate over each file in the directory\n",
    "for book_file in book_files:\n",
    "    with open(book_file, 'r',encoding='UTF-8') as fs:\n",
    "        books.append(fs.read())\n",
    "    print(f'{os.path.basename(book_file)}', len(books[-1]))\n",
    "print()\n",
    "#print(books[0][0:1000])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 - A Face in the Crowd.txt 109317\n",
      "2001 - Dreamcatcher.txt 1229984\n",
      "1982 - The Running Man.txt 400312\n",
      "1991 - Needful Things.txt 1391839\n",
      "2010 - Full Dark No Stars.txt 729712\n",
      "1984 - Thinner.txt 535048\n",
      "1983 - Cycle of the Werewolf.txt 88213\n",
      "1983 - Pet Sematary.txt 795603\n",
      "2004 - Song of Susannah.txt 707473\n",
      "1996 - Desperation.txt 1065799\n",
      "2005 - The Colorado Kid.txt 200947\n",
      "1998 - Bag of Bones.txt 1091002\n",
      "2008 - Duma Key.txt 1114311\n",
      "1986 - It.txt 2457994\n",
      "1999 - Hearts In Atlantis.txt 1013064\n",
      "1995 - Rose Madder.txt 960452\n",
      "1992 - Geralds Game.txt 693212\n",
      "2004 - The Dark Tower.txt 1528909\n",
      "2010 - UR.txt 120843\n",
      "1981 - The Mist.txt 278170\n",
      "2008 - Just After Sunset.txt 741047\n",
      "2012 - In the Tall Grass.txt 108462\n",
      "1987 - The Drawing Of The Three.txt 712126\n",
      "1980 - Firestarter.txt 845812\n",
      "1996 - The Regulators.txt 652027\n",
      "2007 - Blaze.txt 453823\n",
      "2011 - 11_22_63.txt 1485832\n",
      "1975 - Salems Lot.txt 1071564\n",
      "2006 - Cell.txt 676291\n",
      "1984 - The Talisman.txt 1515080\n",
      "1977 - Rage.txt 292880\n",
      "2002 - Everythings Eventual.txt 909284\n",
      "1997 - Wizard and Glass.txt 1441923\n",
      "1979 - The Long Walk.txt 491826\n",
      "1983 - Christine.txt 1064262\n",
      "2003 - Wolves of the Calla.txt 1371319\n",
      "1992 - Dolores Claiborne.txt 494364\n",
      "1981 - Cujo.txt 655553\n",
      "1999 - The Girl Who Loved Tom Gordon.txt 342120\n",
      "2009 - Under the Dome.txt 1884827\n",
      "1996 - The Green Mile.txt 724146\n",
      "2003 - Uncollected Stories 2003.txt 405888\n",
      "1987 - The Tommyknockers.txt 1439309\n",
      "1997 - Six Stories.txt 242988\n",
      "1991 - The Stand.txt 2568498\n",
      "1994 - Insomnia.txt 1340566\n",
      "2002 - From A Buick 8.txt 682727\n",
      "2011 - Mile 81.txt 118095\n",
      "1982 - Different Seasons.txt 1095427\n",
      "2006 - Liseys Story.txt 1027299\n",
      "1993 - Nightmares and Dreamscapes.txt 1511182\n",
      "1974 - Carrie.txt 340261\n",
      "2010 - Blockade Billy.txt 84135\n",
      "1987 - The Eyes of the Dragon.txt 570146\n",
      "2001 - Black House.txt 1394224\n",
      "1979 - The Dead Zone.txt 841313\n",
      "1991 - The Waste Lands.txt 987742\n",
      "1978 - The Shining.txt 916883\n",
      "2012 - The Wind Through The Keyhole.txt 495016\n",
      "1981 - Roadwork.txt 514011\n",
      "1990 - Four Past Midnight.txt 1626811\n",
      "1982 - The Gunslinger.txt 377086\n",
      "1978 - Night Shift.txt 694287\n",
      "1985 - Skeleton Crew.txt 1167121\n",
      "1989 - The Dark Half.txt 864748\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:49:58.239537Z",
     "start_time": "2024-08-16T10:49:57.597149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the text by removing special characters, tags, etc.\"\"\"\n",
    "    # Replace newlines and carriage returns with a space\n",
    "    #text = text.replace('\\n', ' ')\n",
    "    #text = text.replace('\\n\\n', '\\n')\n",
    "    # Remove all non-alphanumeric characters except spaces\n",
    "    #text = re.sub(r'[^A-Za-z0-9\\s]+', ' ', text)\n",
    "    #text = re.sub('<[^>]+>%/', ' ', text)\n",
    "    #text = re.sub('<[^>]+>', '', text) # Remove any character that is not a letter, number, punctuation, or whitespace \n",
    "    #text = re.sub(r'[^A-Za-z0-9\\s.,!?;:()-]', '', text)\n",
    "    text = re.sub(r'\\.{3,}\\s*', '.', text)\n",
    "    # Remove leading tabs and extra spaces\n",
    "    #text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "cleaned_books =[clean_text(book) for book in books]"
   ],
   "id": "d1b68df037cd55c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "34c81666-2587-4b6f-99dd-ad746515cb30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T11:08:58.586887Z",
     "start_time": "2024-08-16T11:08:58.383994Z"
    }
   },
   "source": [
    "# Some long paragraphs will be truncated during training. \n",
    "# To avoid lossing data, we split them into shorter ones.\n",
    "\n",
    "import nltk\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def split_text_at_sentences(text, max_tokens=4000):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        \n",
    "        # Check if adding this sentence would exceed the max token limit\n",
    "        if len(current_tokens) + len(sentence_tokens) > max_tokens:\n",
    "            # If so, finalize the current chunk and start a new one\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_tokens = []\n",
    "        \n",
    "        # Add the sentence to the current chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_tokens.extend(sentence_tokens)\n",
    "    \n",
    "    # Add the last chunk if there's any content left\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T11:10:48.688153Z",
     "start_time": "2024-08-16T11:08:59.647068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunked_books = []\n",
    "for i, book in enumerate(cleaned_books):\n",
    "    chunked_book = ''\n",
    "    book='\\n'.join(line for line in book.splitlines() if line.strip())\n",
    "    chunks = split_text_at_sentences(book.replace(\"\\n\", \" \"))\n",
    "    for chunk in chunks:\n",
    "        chunked_book += chunk + \"\\n\"\n",
    "    chunked_books.append(chunked_book)"
   ],
   "id": "38e4b1047d29ce34",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1199 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "72859f74-44c3-4e7b-9e84-77493959d57d",
   "metadata": {},
   "source": [
    "# Merge to train and eval"
   ]
  },
  {
   "cell_type": "code",
   "id": "7529b175-53e3-4991-a59c-d1772faf1e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T11:11:51.690721Z",
     "start_time": "2024-08-16T11:11:51.353096Z"
    }
   },
   "source": [
    "num_train_books = 17\n",
    "train = ''\n",
    "eval =''\n",
    "for i in range(num_train_books):\n",
    "    train += chunked_books[i]\n",
    "for j in range(len(chunked_books)-num_train_books):    \n",
    "    eval += chunked_books[num_train_books+j]\n",
    "\n",
    "with open(f'data_files/train.txt', 'w') as fs:\n",
    "    fs.write(train)\n",
    "\n",
    "with open(f'data_files/eval.txt', 'w') as fs:\n",
    "    fs.write(eval)\n",
    "\n",
    "with open(f'data_files/full.txt', 'w') as fs:\n",
    "    fs.write(train + eval)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "02bbff36-8d80-498c-8d28-bf44ef49dff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T11:11:52.691210Z",
     "start_time": "2024-08-16T11:11:52.526051Z"
    }
   },
   "source": [
    "def count_lines_in_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        line_count = sum(1 for line in file)\n",
    "    return line_count\n",
    " \n",
    "# Example usage:\n",
    "filename = 'your_text_file.txt'\n",
    "train_number_of_lines = count_lines_in_file('data_files/train.txt')\n",
    "full_number_of_lines = count_lines_in_file('data_files/full.txt')\n",
    "eval_number_of_lines = count_lines_in_file('data_files/eval.txt')\n",
    "print(f'The train file has {train_number_of_lines} lines.')\n",
    "print(f'The full file has {full_number_of_lines} lines.')\n",
    "print(f'The eval file has {eval_number_of_lines} lines.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train file has 941 lines.\n",
      "The full file has 3588 lines.\n",
      "The eval file has 2647 lines.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6e0794c230ab404"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
