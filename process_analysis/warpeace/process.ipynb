{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f52dd73-8aa4-4fae-91b2-cf244423d83d",
   "metadata": {},
   "source": [
    "# Load books"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8ddc3bd-ea79-4690-9650-70d22dd38d36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T12:20:50.304103Z",
     "start_time": "2024-08-18T12:20:50.294288Z"
    }
   },
   "source": [
    "import glob\n",
    "import os\n",
    "books_directory = '/home/ubuntu/Downloads/twiker-experiments/process_analysis/warpeace/books'\n",
    "books = []\n",
    "\n",
    "# List all .txt files in the directory using glob\n",
    "book_files = glob.glob(os.path.join(books_directory, '*.txt'))\n",
    "# Iterate over each file in the directory\n",
    "for book_file in book_files:\n",
    "    with open(book_file, 'r',encoding='UTF-8') as fs:\n",
    "        books.append(fs.read())\n",
    "    print(f'{os.path.basename(book_file)}', len(books[-1]))\n",
    "print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book-war-and-peace.txt 3202303\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T12:20:51.389705Z",
     "start_time": "2024-08-18T12:20:51.380165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the text by removing special characters, tags, etc.\"\"\"\n",
    "    # Replace newlines and carriage returns with a space\n",
    "    text = text.replace('\\n\\n', '\\n')\n",
    "    text = re.sub('<[^>]+>', ' ', text)\n",
    "    return text\n",
    "\n",
    "cleaned_books =[clean_text(book) for book in books]"
   ],
   "id": "6abc7612a88b9929",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "34c81666-2587-4b6f-99dd-ad746515cb30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T12:20:52.813770Z",
     "start_time": "2024-08-18T12:20:52.505181Z"
    }
   },
   "source": [
    "# Some long paragraphs will be truncated during training. \n",
    "# To avoid lossing data, we split them into shorter ones.\n",
    "\n",
    "import nltk\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def split_text_at_sentences(text, max_tokens=500):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        \n",
    "        # Check if adding this sentence would exceed the max token limit\n",
    "        if len(current_tokens) + len(sentence_tokens) > max_tokens:\n",
    "            # If so, finalize the current chunk and start a new one\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_tokens = []\n",
    "        \n",
    "        # Add the sentence to the current chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_tokens.extend(sentence_tokens)\n",
    "    \n",
    "    # Add the last chunk if there's any content left\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "f204e2da-7ae4-4bf3-be71-bfbd798638b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T12:21:00.015562Z",
     "start_time": "2024-08-18T12:20:53.618571Z"
    }
   },
   "source": [
    "chucked_books = []\n",
    "for i, book in enumerate(cleaned_books):\n",
    "    chunked_book = ''\n",
    "    chunks = split_text_at_sentences(book.replace(\"\\n\", \" \"))\n",
    "    for chunk in chunks:\n",
    "        chunked_book += chunk + \"\\n\"\n",
    "    chucked_books.append(chunked_book)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "72859f74-44c3-4e7b-9e84-77493959d57d",
   "metadata": {},
   "source": [
    "# Merge to train and eval"
   ]
  },
  {
   "cell_type": "code",
   "id": "7529b175-53e3-4991-a59c-d1772faf1e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T12:21:04.442278Z",
     "start_time": "2024-08-18T12:21:04.420724Z"
    }
   },
   "source": [
    "# Assuming `chucked_books` is a list where each element is a string containing paragraphs of the book.\n",
    "book_content = chucked_books[0]  # Get the entire book content\n",
    " \n",
    "# Assuming paragraphs are already defined (e.g., separated by '\\n\\n').\n",
    "paragraphs = book_content.split('\\n')\n",
    " \n",
    "# Calculate the split index for 80% of the paragraphs for training\n",
    "split_index = int(0.65 * len(paragraphs))\n",
    " \n",
    "# Check if split_index is at a paragraph boundary\n",
    "train_paragraphs = paragraphs[:split_index]  # First 80% of the paragraphs for training\n",
    "eval_paragraphs = paragraphs[split_index:]   # Remaining 20% of the paragraphs for evaluation\n",
    " \n",
    "# Ensure that paragraphs are not split inappropriately\n",
    "# In this case, since we split by paragraph, each set will contain whole paragraphs.\n",
    " \n",
    "# Join the paragraphs back into a single string for train and eval\n",
    "train = '\\n'.join(train_paragraphs)\n",
    "eval = '\\n'.join(eval_paragraphs)\n",
    " \n",
    "# Write the train, eval, and full content to respective files\n",
    "with open('data_files/train.txt', 'w') as fs:\n",
    "    fs.write(train)\n",
    " \n",
    "with open('data_files/eval.txt', 'w') as fs:\n",
    "    fs.write(eval)\n",
    " \n",
    "with open('data_files/full.txt', 'w') as fs:\n",
    "    fs.write(book_content)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "02bbff36-8d80-498c-8d28-bf44ef49dff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T11:14:49.082280Z",
     "start_time": "2024-08-16T11:14:49.068763Z"
    }
   },
   "source": [
    "def count_lines_in_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        line_count = sum(1 for line in file)\n",
    "    return line_count\n",
    " \n",
    "# Example usage:\n",
    "filename = 'your_text_file.txt'\n",
    "train_number_of_lines = count_lines_in_file('data_files/train.txt')\n",
    "full_number_of_lines = count_lines_in_file('data_files/full.txt')\n",
    "eval_number_of_lines = count_lines_in_file('data_files/eval.txt')\n",
    "print(f'The train file has {train_number_of_lines} lines.')\n",
    "print(f'The full file has {full_number_of_lines} lines.')\n",
    "print(f'The eval file has {eval_number_of_lines} lines.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train file has 968 lines.\n",
      "The full file has 1489 lines.\n",
      "The eval file has 521 lines.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7e12dbdd5f6d065f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
