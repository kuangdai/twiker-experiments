{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f52dd73-8aa4-4fae-91b2-cf244423d83d",
   "metadata": {},
   "source": [
    "# Load books"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8ddc3bd-ea79-4690-9650-70d22dd38d36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T09:52:19.173496Z",
     "start_time": "2024-08-16T09:52:19.164416Z"
    }
   },
   "source": [
    "import glob\n",
    "import os\n",
    " \n",
    "# Specify the top-level directory containing all subdirectories\n",
    "books_directory = '/home/ubuntu/Downloads/twiker-experiments/process_analysis/CRIMEandPUNISHMENT/books'\n",
    "books = []\n",
    "\n",
    "# List all .txt files in the directory using glob\n",
    "book_files = glob.glob(os.path.join(books_directory, '*.txt'))\n",
    "# Iterate over each file in the directory\n",
    "for book_file in book_files:\n",
    "    with open(book_file, 'r',encoding='UTF-8') as fs:\n",
    "        books.append(fs.read())\n",
    "    print(f'{os.path.basename(book_file)}', len(books[-1]))\n",
    "print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIME AND PUNISHMENT.txt 1129664\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T09:52:20.810172Z",
     "start_time": "2024-08-16T09:52:20.736279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the text by removing special characters, tags, etc.\"\"\"\n",
    "    # Replace newlines and carriage returns with a space\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Remove all non-alphanumeric characters except spaces\n",
    "    #text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "    #text = re.sub('<[^>]+>%/', '', text)\n",
    "    text = re.sub('<[^>]+>', '', text) # Remove any character that is not a letter, number, punctuation, or whitespace \n",
    "    #text = re.sub(r'[^A-Za-z0-9\\s.,!?;:()-]', '', text)\n",
    "    #text = re.sub(r'\\.{3,}\\s*', '', text)\n",
    "    # Remove leading tabs and extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "cleaned_books =[clean_text(book) for book in books]"
   ],
   "id": "3d30e80e70530a7a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "34c81666-2587-4b6f-99dd-ad746515cb30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T09:52:24.784411Z",
     "start_time": "2024-08-16T09:52:21.992251Z"
    }
   },
   "source": [
    "# Some long paragraphs will be truncated during training. \n",
    "# To avoid lossing data, we split them into shorter ones.\n",
    "\n",
    "import nltk\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def split_text_at_sentences(text, max_tokens=200):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        \n",
    "        # Check if adding this sentence would exceed the max token limit\n",
    "        if len(current_tokens) + len(sentence_tokens) > max_tokens:\n",
    "            # If so, finalize the current chunk and start a new one\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_tokens = []\n",
    "        \n",
    "        # Add the sentence to the current chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_tokens.extend(sentence_tokens)\n",
    "    \n",
    "    # Add the last chunk if there's any content left\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/ubuntu/miniconda3/envs/transformers-twiker/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T09:52:28.888523Z",
     "start_time": "2024-08-16T09:52:26.209285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chucked_books = []\n",
    "for i, book in enumerate(cleaned_books):\n",
    "    chunked_book = ''\n",
    "    chunks = split_text_at_sentences(book.replace(\"\\n\", \" \"))\n",
    "    for chunk in chunks:\n",
    "        chunked_book += chunk + \"\\n\"\n",
    "    chucked_books.append(chunked_book)"
   ],
   "id": "b879f4e63f8c1ae0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "72859f74-44c3-4e7b-9e84-77493959d57d",
   "metadata": {},
   "source": [
    "# Merge to train and eval"
   ]
  },
  {
   "cell_type": "code",
   "id": "7529b175-53e3-4991-a59c-d1772faf1e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T09:52:30.982131Z",
     "start_time": "2024-08-16T09:52:30.968319Z"
    }
   },
   "source": [
    "# Assuming `chucked_books` is a list where each element is a string containing paragraphs of the book.\n",
    "book_content = chucked_books[0]  # Get the entire book content\n",
    " \n",
    "# Assuming paragraphs are already defined (e.g., separated by '\\n\\n').\n",
    "paragraphs = book_content.split('\\n')\n",
    " \n",
    "# Calculate the split index for 80% of the paragraphs for training\n",
    "split_index = int(0.6 * len(paragraphs))\n",
    " \n",
    "# Check if split_index is at a paragraph boundary\n",
    "train_paragraphs = paragraphs[:split_index]  # First 80% of the paragraphs for training\n",
    "eval_paragraphs = paragraphs[split_index:]   # Remaining 20% of the paragraphs for evaluation\n",
    " \n",
    "# Ensure that paragraphs are not split inappropriately\n",
    "# In this case, since we split by paragraph, each set will contain whole paragraphs.\n",
    " \n",
    "# Join the paragraphs back into a single string for train and eval\n",
    "train = '\\n'.join(train_paragraphs)\n",
    "eval = '\\n'.join(eval_paragraphs)\n",
    " \n",
    "# Write the train, eval, and full content to respective files\n",
    "with open('data_files/train.txt', 'w') as fs:\n",
    "    fs.write(train)\n",
    " \n",
    "with open('data_files/eval.txt', 'w') as fs:\n",
    "    fs.write(eval)\n",
    " \n",
    "with open('data_files/full.txt', 'w') as fs:\n",
    "    fs.write(book_content)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "02bbff36-8d80-498c-8d28-bf44ef49dff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T13:46:50.684282Z",
     "start_time": "2024-08-11T13:46:50.681443Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f7948a8580bbf6a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
