{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f52dd73-8aa4-4fae-91b2-cf244423d83d",
   "metadata": {},
   "source": [
    "# Load books"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8ddc3bd-ea79-4690-9650-70d22dd38d36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:00:12.590588Z",
     "start_time": "2024-08-16T10:00:12.445088Z"
    }
   },
   "source": [
    "import glob\n",
    "import os\n",
    " \n",
    "# Specify the top-level directory containing all subdirectories\n",
    "books_directory = '/home/ubuntu/Downloads/twiker-experiments/process_analysis/dikens/books'\n",
    "books = []\n",
    "\n",
    "# List all .txt files in the directory using glob\n",
    "book_files = glob.glob(os.path.join(books_directory, '*.txt'))\n",
    "# Iterate over each file in the directory\n",
    "for book_file in book_files:\n",
    "    with open(book_file, 'r') as fs:\n",
    "        books.append(fs.read())\n",
    "    print(f'{os.path.basename(book_file)}', len(books[-1]))\n",
    "print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dickens.txt 44797976\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:00:14.085284Z",
     "start_time": "2024-08-16T10:00:14.043264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the text by removing special characters, tags, etc.\"\"\"\n",
    "    # Replace newlines and carriage returns with a space\n",
    "    #text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Remove all non-alphanumeric characters except spaces\n",
    "    #text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "    #text = re.sub('<[^>]+>%/', '', text)\n",
    "    text = re.sub('<[^>]+>', '', text) # Remove any character that is not a letter, number, punctuation, or whitespace \n",
    "    #text = re.sub(r'[^A-Za-z0-9\\s.,!?;:()-]', '', text)\n",
    "    #text = re.sub(r'\\.{3,}\\s*', '', text)\n",
    "    # Remove leading tabs and extra spaces\n",
    "    #text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "cleaned_books =[clean_text(book) for book in books]"
   ],
   "id": "3d30e80e70530a7a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "34c81666-2587-4b6f-99dd-ad746515cb30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:28:33.816759Z",
     "start_time": "2024-08-16T10:28:33.585024Z"
    }
   },
   "source": [
    "# Some long paragraphs will be truncated during training. \n",
    "# To avoid lossing data, we split them into shorter ones.\n",
    "\n",
    "import nltk\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def split_text_at_sentences(text, max_tokens=7000):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        \n",
    "        # Check if adding this sentence would exceed the max token limit\n",
    "        if len(current_tokens) + len(sentence_tokens) > max_tokens:\n",
    "            # If so, finalize the current chunk and start a new one\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_tokens = []\n",
    "        \n",
    "        # Add the sentence to the current chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_tokens.extend(sentence_tokens)\n",
    "    \n",
    "    # Add the last chunk if there's any content left\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:29:47.441240Z",
     "start_time": "2024-08-16T10:28:34.787727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chucked_books = []\n",
    "for i, book in enumerate(books):\n",
    "    chunked_book = ''\n",
    "    chunks = split_text_at_sentences(book.replace(\"\\n\", \" \"))\n",
    "    for chunk in chunks:\n",
    "        chunked_book += chunk + \"\\n\"\n",
    "    chucked_books.append(chunked_book)"
   ],
   "id": "258a9d6abc5b50ec",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:30:28.572501Z",
     "start_time": "2024-08-16T10:30:28.567195Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(chunked_book))",
   "id": "a91f62b9525fddad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44641031\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "72859f74-44c3-4e7b-9e84-77493959d57d",
   "metadata": {},
   "source": [
    "# Merge to train and eval"
   ]
  },
  {
   "cell_type": "code",
   "id": "7529b175-53e3-4991-a59c-d1772faf1e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:30:30.291280Z",
     "start_time": "2024-08-16T10:30:30.016257Z"
    }
   },
   "source": [
    "# Assuming `chucked_books` is a list where each element is a string containing paragraphs of the book.\n",
    "book_content = chucked_books[0]  # Get the entire book content\n",
    " \n",
    "# Assuming paragraphs are already defined (e.g., separated by '\\n\\n').\n",
    "paragraphs = book_content.split('\\n')\n",
    " \n",
    "# Calculate the split index for 80% of the paragraphs for training\n",
    "split_index = int(0.55 * len(paragraphs))\n",
    " \n",
    "# Check if split_index is at a paragraph boundary\n",
    "train_paragraphs = paragraphs[:split_index]  # First 80% of the paragraphs for training\n",
    "eval_paragraphs = paragraphs[split_index:]   # Remaining 20% of the paragraphs for evaluation\n",
    " \n",
    "# Ensure that paragraphs are not split inappropriately\n",
    "# In this case, since we split by paragraph, each set will contain whole paragraphs.\n",
    " \n",
    "# Join the paragraphs back into a single string for train and eval\n",
    "train = '\\n'.join(train_paragraphs)\n",
    "eval = '\\n'.join(eval_paragraphs)\n",
    " \n",
    "# Write the train, eval, and full content to respective files\n",
    "with open('data_files/train.txt', 'w') as fs:\n",
    "    fs.write(train)\n",
    " \n",
    "with open('data_files/eval.txt', 'w') as fs:\n",
    "    fs.write(eval)\n",
    " \n",
    "with open('data_files/full.txt', 'w') as fs:\n",
    "    fs.write(book_content)"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:30:31.350935Z",
     "start_time": "2024-08-16T10:30:31.267694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_lines_in_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        line_count = sum(1 for line in file)\n",
    "    return line_count\n",
    " \n",
    "# Example usage:\n",
    "filename = 'your_text_file.txt'\n",
    "train_number_of_lines = count_lines_in_file('data_files/train.txt')\n",
    "full_number_of_lines = count_lines_in_file('data_files/full.txt')\n",
    "print(f'The train file has {train_number_of_lines} lines.')\n",
    "print(f'The full file has {full_number_of_lines} lines.')"
   ],
   "id": "726fb0d3f93cd0b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train file has 872 lines.\n",
      "The full file has 1586 lines.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e17f4aa20f0c1d4f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
