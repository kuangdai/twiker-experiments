{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f52dd73-8aa4-4fae-91b2-cf244423d83d",
   "metadata": {},
   "source": [
    "# Load books"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8ddc3bd-ea79-4690-9650-70d22dd38d36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:40:07.783277Z",
     "start_time": "2024-08-16T10:40:07.762422Z"
    }
   },
   "source": [
    "books = []\n",
    "\n",
    "for i in range(1, 8):\n",
    "    with open(f'books/{i}.txt', 'r') as fs:\n",
    "        books.append(fs.read())\n",
    "    print(f'{i}.txt', len(books[-1]))\n",
    "\n",
    "print()\n",
    "print(books[0][0:1000])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt 439478\n",
      "2.txt 492297\n",
      "3.txt 625735\n",
      "4.txt 1100515\n",
      "5.txt 1499410\n",
      "6.txt 987992\n",
      "7.txt 1140011\n",
      "\n",
      "M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.\n",
      "\n",
      "Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\n",
      "\n",
      "The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn’t think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley’s sister, but they hadn’t met for several ye\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "34c81666-2587-4b6f-99dd-ad746515cb30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:43:14.292954Z",
     "start_time": "2024-08-16T10:43:14.092348Z"
    }
   },
   "source": [
    "# Some long paragraphs will be truncated during training. \n",
    "# To avoid lossing data, we split them into shorter ones.\n",
    "\n",
    "import nltk\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def split_text_at_sentences(text, max_tokens=700):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        \n",
    "        # Check if adding this sentence would exceed the max token limit\n",
    "        if len(current_tokens) + len(sentence_tokens) > max_tokens:\n",
    "            # If so, finalize the current chunk and start a new one\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_tokens = []\n",
    "        \n",
    "        # Add the sentence to the current chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_tokens.extend(sentence_tokens)\n",
    "    \n",
    "    # Add the last chunk if there's any content left\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "f204e2da-7ae4-4bf3-be71-bfbd798638b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:43:27.376509Z",
     "start_time": "2024-08-16T10:43:15.204395Z"
    }
   },
   "source": [
    "chucked_books = []\n",
    "for i, book in enumerate(books):\n",
    "    chunked_book = ''\n",
    "    chunks = split_text_at_sentences(book.replace(\"\\n\", \" \"))\n",
    "    for chunk in chunks:\n",
    "        chunked_book += chunk + \"\\n\"\n",
    "    chucked_books.append(chunked_book)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "72859f74-44c3-4e7b-9e84-77493959d57d",
   "metadata": {},
   "source": [
    "# Merge to train and eval"
   ]
  },
  {
   "cell_type": "code",
   "id": "7529b175-53e3-4991-a59c-d1772faf1e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:44:00.541665Z",
     "start_time": "2024-08-16T10:44:00.504182Z"
    }
   },
   "source": [
    "num_train_books = 4\n",
    "train = ''\n",
    "eval =''\n",
    "for i in range(num_train_books):\n",
    "    train += chucked_books[i]\n",
    "for j in range(len(chucked_books)-num_train_books):    \n",
    "    eval += chucked_books[num_train_books+j]\n",
    "\n",
    "with open(f'data_files/train.txt', 'w') as fs:\n",
    "    fs.write(train)\n",
    "\n",
    "with open(f'data_files/eval.txt', 'w') as fs:\n",
    "    fs.write(eval)\n",
    "\n",
    "with open(f'data_files/full.txt', 'w') as fs:\n",
    "    fs.write(train + eval)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "02bbff36-8d80-498c-8d28-bf44ef49dff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:44:01.528364Z",
     "start_time": "2024-08-16T10:44:01.499326Z"
    }
   },
   "source": [
    "def count_lines_in_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        line_count = sum(1 for line in file)\n",
    "    return line_count\n",
    " \n",
    "# Example usage:\n",
    "filename = 'your_text_file.txt'\n",
    "train_number_of_lines = count_lines_in_file('data_files/train.txt')\n",
    "full_number_of_lines = count_lines_in_file('data_files/full.txt')\n",
    "eval_number_of_lines = count_lines_in_file('data_files/eval.txt')\n",
    "print(f'The train file has {train_number_of_lines} lines.')\n",
    "print(f'The full file has {full_number_of_lines} lines.')\n",
    "print(f'The eval file has {eval_number_of_lines} lines.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train file has 1033 lines.\n",
      "The full file has 2370 lines.\n",
      "The eval file has 1337 lines.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7d1294d66a766f49"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
