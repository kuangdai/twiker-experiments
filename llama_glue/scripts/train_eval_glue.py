import os

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import argparse
import json
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, TrainingArguments
from datasets import load_dataset
from evaluate import load as load_metric
import numpy as np
import sys
from peft import LoraConfig, get_peft_model

from transformers import Trainer
import torch.nn as nn
from transformers import TrainerCallback


class EvalLoggerCallback(TrainerCallback):
    def __init__(self, save_dir, task, config_name):
        self.save_dir = save_dir
        self.task = task
        self.config_name = config_name

    def on_evaluate(self, args, state, control, metrics, **kwargs):
        os.makedirs(self.save_dir, exist_ok=True)
        save_path = os.path.join(
            self.save_dir,
            f"{self.task}_{self.config_name}_epoch{int(state.epoch)}.txt"
        )

        with open(save_path, "w") as f:
            f.write(f"ğŸŒŸ Evaluation results for {self.task} | Config: {self.config_name} | Epoch: {state.epoch}\n")
            for k, v in metrics.items():
                f.write(f"{k}: {v:.4f}\n")

        print(f"âœ… Eval results saved to {save_path}")


class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits

        if args.task == "stsb":
            loss = nn.MSELoss()(logits.view(-1), labels.view(-1))
        else:
            # ç»Ÿä¸€ç”¨CrossEntropyLossï¼Œé€‚ç”¨äºäºŒåˆ†ç±»å’Œå¤šåˆ†ç±»
            loss = nn.CrossEntropyLoss()(logits, labels)

        return (loss, outputs) if return_outputs else loss


# å‚æ•°è§£æ
parser = argparse.ArgumentParser()
parser.add_argument("--task", required=True, type=str)
parser.add_argument("--epochs", required=True, type=int)
parser.add_argument("--twiker-config", required=True, type=str)
parser.add_argument("--max-steps", default=-1, type=int)
args = parser.parse_args()

print("âœ… å‚æ•°è§£æå®Œæˆ")
sys.stdout.flush()

# å†…ç½®è·¯å¾„
DATA_ROOT = "../datasets"
MODEL_ROOT = "../models/llama-8b"
OUTPUT_ROOT = f"../outputs/{args.task}/twiker_results"

print("âœ… ç›®å½•è®¾ç½®å®Œæˆ")
sys.stdout.flush()

# è¯»å–TWiKeré…ç½®
print(f"âœ… æ­£åœ¨è¯»å–é…ç½®æ–‡ä»¶: {args.twiker_config}")
with open(args.twiker_config, "r") as f:
    twiker_cfg = json.load(f)

# åŠ è½½é…ç½®ä¸æ¨¡å‹
print("âœ… åŠ è½½Tokenizerä¸é…ç½®")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ROOT)
tokenizer.pad_token = tokenizer.eos_token
config = AutoConfig.from_pretrained(MODEL_ROOT)
for k, v in twiker_cfg.items():
    setattr(config, k, v)
config.pad_token_id = tokenizer.pad_token_id
if args.task == "stsb":
    config.num_labels = 1
elif args.task == "mnli":
    config.num_labels = 3
else:
    config.num_labels = 2

print("âœ… åŠ è½½æ¨¡å‹")
model = AutoModelForSequenceClassification.from_pretrained(MODEL_ROOT, config=config)

# åŠ¨æ€ç¡®å®šä¿ç•™æ¢¯åº¦æ¨¡å—
additional_modules = []
if getattr(config, "twiker_activated", False):
    additional_modules.append("twiker_model.embedding")

# é…ç½®LoRA
print(f"âœ… ä¿ç•™æ¢¯åº¦æ¨¡å— (ä¸åŠ LoRA): {additional_modules}")
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    modules_to_save=additional_modules,
    bias="none",
    task_type="SEQ_CLS"
)

# æ³¨å…¥
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

print("âœ… å¯ä¼˜åŒ–å‚æ•°æ¨¡å— (ç»Ÿè®¡æ€»å…ƒç´ ä¸ªæ•°):")
for name, module in model.named_modules():
    param_list = list(module.parameters(recurse=False))
    if not param_list:
        continue

    total_elements = sum(p.numel() for p in param_list)
    trainable_elements = sum(p.numel() for p in param_list if p.requires_grad)

    if trainable_elements > 0 and "modules_to_save" in name:
        print(f"{name} | æ€»å‚æ•°å…ƒç´ : {total_elements} | å¯è®­ç»ƒå…ƒç´ : {trainable_elements}")

# æ•°æ®åŠ è½½
print("âœ… åŠ è½½æ•°æ®é›†")
dataset = load_dataset("json", data_files={
    "train": os.path.join(DATA_ROOT, args.task, "train.jsonl"),
    "validation": os.path.join(DATA_ROOT, args.task, "validation.jsonl")
})

# é¢„å¤„ç†
print("âœ… å¼€å§‹æ•°æ®é¢„å¤„ç†")


def preprocess(examples):
    if args.task in ["sst2", "cola"]:
        return tokenizer(examples["sentence"], truncation=True, padding="max_length", max_length=128)

    elif args.task in ["rte", "stsb", "wnli", "mrpc"]:
        return tokenizer(examples["sentence1"], examples["sentence2"], truncation=True, padding="max_length",
                         max_length=128)

    elif args.task == "mnli":
        return tokenizer(examples["premise"], examples["hypothesis"], truncation=True, padding="max_length",
                         max_length=128)

    elif args.task == "qnli":
        return tokenizer(examples["question"], examples["sentence"], truncation=True, padding="max_length",
                         max_length=128)

    elif args.task == "qqp":
        return tokenizer(examples["question1"], examples["question2"], truncation=True, padding="max_length",
                         max_length=128)

    else:
        raise NotImplementedError(f"Task {args.task} not supported")


dataset = dataset.map(preprocess, batched=True)
print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
sys.stdout.flush()

# è¯„ä¼°æŒ‡æ ‡
print("âœ… åŠ è½½è¯„ä¼°æŒ‡æ ‡")
glue_metric = load_metric("glue_metrics/glue/glue.py", args.task)


def compute_metrics(eval_pred):
    preds, labels = eval_pred
    if args.task == "stsb":
        preds = np.squeeze(preds)
    else:
        preds = np.argmax(preds, axis=1)
    results = glue_metric.compute(predictions=preds, references=labels)
    return results


# è®­ç»ƒå‚æ•°
print("âœ… é…ç½®è®­ç»ƒå‚æ•°")
training_args = TrainingArguments(
    output_dir=OUTPUT_ROOT,
    evaluation_strategy="epoch",
    learning_rate=1e-4,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=args.epochs,
    save_strategy="no",
    report_to=[],
    fp16=True,
    gradient_accumulation_steps=4,
    max_steps=args.max_steps,
    disable_tqdm=True,
)

# Trainer åˆå§‹åŒ–
# è·å–é…ç½®åï¼Œå»æ‰è·¯å¾„å’Œåç¼€
config_basename = os.path.splitext(os.path.basename(args.twiker_config))[0]

# å›è°ƒå®ä¾‹
eval_logger = EvalLoggerCallback(
    save_dir="../results/eval_epochs",
    task=args.task,
    config_name=config_basename
)

# Trainer
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    compute_metrics=compute_metrics,
    callbacks=[eval_logger],
)

# è®­ç»ƒ
print("âœ… æ­£å¼å¼€å§‹è®­ç»ƒ")
sys.stdout.flush()
trainer.train()

# è¯„ä¼°
print("âœ… è®­ç»ƒå®Œæˆï¼Œå¼€å§‹è¯„ä¼°")
final_metrics = trainer.evaluate()

# è¾“å‡ºä¸ä¿å­˜ç»“æœ
print("âœ… Final Evaluation Results:")
result_dir = f"../results/{args.task}_{os.path.splitext(os.path.basename(args.twiker_config))[0]}"
os.makedirs(result_dir, exist_ok=True)
result_path = os.path.join(result_dir, "metrics.txt")

with open(result_path, "w") as f:
    for k, v in final_metrics.items():
        if isinstance(v, float):
            line = f"{k}: {v:.4f}"
            print(line)
            f.write(line + "\n")

print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {result_path}")
